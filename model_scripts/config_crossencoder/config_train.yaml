data:
  file_path:
    train:
      raw: /home/dghilardi/kgqa/mention-clustering/zeshel_processed/train.json
      biencoder: /home/dghilardi/kgqa/mention-clustering/zeshel_prepared/biencoder/train.pkl
    val:
      raw: /home/dghilardi/kgqa/mention-clustering/zeshel_processed/val.json
      biencoder: /home/dghilardi/kgqa/mention-clustering/zeshel_prepared/biencoder/val.pkl
    test:
      raw: /home/dghilardi/kgqa/mention-clustering/zeshel_processed/test.json
      biencoder: /home/dghilardi/kgqa/mention-clustering/zeshel_prepared/biencoder/test.pkl
  output_path: /home/dghilardi/kgqa/mention-clustering/zeshel_prepared/crossencoder/
  mode: Load #Create, Load
  batch: 64
  dataset_name: CrossEncoderDataset

tokenizer:
  name: CrossEncoderTokenizer
  model_ckpt: bert-base-uncased
  params:
    max_length: 256
    padding: max_length
    truncation: True
    max_word_context: 30
  
encoder:
  name: CLSPooling
  model_ckpt: bert-base-uncased
  params:
    output_hidden_states: True 

loss:
  name: BinaryCrossEntropyLoss
  params:
    pos_weight: 5

optimizer:
  learning_rate: 1e-5

trainer:
  accelerator: cuda
  # enable_checkpointing: False
  deterministic: True
  precision: 16-mixed
  log_every_n_steps: 0.1
  val_check_interval: 0.1
  max_epochs: 2
  # strategy: ddp_find_unused_parameters_true
  inference_mode: False
  devices: [3]
  fast_dev_run: False

seed: 11

wandb:
  enable: True
  params:
    name: CrossEncoder
    project: CrossEncoder Zeshel
    log_model: False

early_stopping:
  enable: False
  params:
    monitor: val_loss
    patience: 3
    mode: min

model_checkpoint:
  enable: True
  params:
    dirpath: /home/dghilardi/kgqa/mention-clustering/biencoder/zeshel/CrossEncoder/
    monitor: val_loss
    mode: min